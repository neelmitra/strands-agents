{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 06 Observability with LangFuse and Evaluation with RAGAS ðŸ”ðŸ“Š\n",
    "\n",
    "In the Strands Agents SDK, observability refers to your ability to measure system behavior and performance. Observability combines instrumentation, data collection, and analysis techniques. These techniques provide insights into an agent's behavior and performance, helping you effectively build, debug, and maintain agents that better serve your unique needs and reliably complete tasks.\n",
    "\n",
    "This notebook demonstrates how to build an agent with observability and evaluation capabilities. \n",
    "\n",
    "We use [Langfuse](https://langfuse.com/) to process the Strands Agent traces and [Ragas](https://www.ragas.io/) metrics to evaluate agent performance. The primary focus is on agent evaluation and the quality of responses generated by the agent using traces produced by the SDK.\n",
    "\n",
    "### What is Observability and Evaluation?\n",
    "\n",
    "**Observability** means being able to see what your AI agent is doing \"behind the scenes\" - like watching its thought process. It helps you understand why your agent makes certain decisions or gives particular responses.\n",
    "\n",
    "**Evaluation** is how we measure if our agent is doing a good job. Instead of just guessing if responses are good, we use specific metrics to score the agent's performance.\n",
    "\n",
    "### Observability Components\n",
    "\n",
    "All observability APIs are embedded directly within the Strands Agents SDK. The following are key observability data points:\n",
    "\n",
    "[**Metrics**](https://strandsagents.com/latest/user-guide/observability-evaluation/metrics/) - Essential for understanding agent performance, optimizing behavior, and monitoring resource usage.\n",
    "\n",
    "[**Traces**](https://strandsagents.com/latest/user-guide/observability-evaluation/traces/) - A fundamental component of the Strands SDK's observability framework, providing detailed insights into your agent's execution.\n",
    "\n",
    "[**Logs**](https://strandsagents.com/latest/user-guide/observability-evaluation/logs/) - Strands SDK uses Python's standard logging module to provide visibility into operations.\n",
    "\n",
    "[**Evaluation**](https://strandsagents.com/latest/user-guide/observability-evaluation/evaluation/) - Essential for measuring agent performance, tracking improvements, and ensuring your agents meet quality standards. With Strands SDK, you can perform Manual Evaluation, Structured Testing, LLM Judge Evaluation, and Tool-Specific Evaluation.\n",
    "\n",
    "### OpenTelemetry Integration\n",
    "\n",
    "Strands natively integrates with OpenTelemetry, an industry standard for distributed tracing. You can visualize and analyze traces using any OpenTelemetry-compatible tool. This integration provides:\n",
    "\n",
    "- **Compatibility with existing observability tools:** Send traces to platforms such as Jaeger, Grafana Tempo, AWS X-Ray, Datadog, and more\n",
    "- **Standardized attribute naming:** Uses OpenTelemetry semantic conventions\n",
    "- **Flexible export options:** Console output for development, OTLP endpoint for production\n",
    "- **Auto-instrumentation:** Trace creation is handled automatically when you turn on tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ½ï¸ðŸ”Observability and Evaluation with Restaurant Agent\n",
    "\n",
    "In this notebook, we'll demonstrate how to build a restaurant recommendation agent with observability and evaluation capabilities. This is designed for beginners who want to learn about AI agents, observability, and evaluation without complex infrastructure setup.\n",
    "\n",
    "> â­ Based on the code from [08-observability-and-evaluation/Observability-and-Evaluation-sample.ipynb](https://github.com/strands-agents/samples/blob/main/01-tutorials/01-fundamentals/08-observability-and-evaluation/Observability-and-Evaluation-sample.ipynb) of the [Strands Agents Samples repository](https://github.com/strands-agents/)\n",
    "\n",
    "### What We'll Build\n",
    "\n",
    "We'll use these key components:\n",
    "\n",
    "1. **Local Vector Database**: A searchable collection of restaurant information that our agent can query\n",
    "2. **Strands Agent**: An AI assistant that can recommend restaurants based on user preferences\n",
    "3. **LangFuse**: A tool that lets us \"see\" how our agent works and makes decisions\n",
    "4. **RAGAS**: A framework that helps us evaluate how well our agent is performing\n",
    "\n",
    "This approach is beginner-friendly and doesn't require complex AWS infrastructure while still providing a comprehensive learning experience.\n",
    "\n",
    "![image](image/restaurant_agent_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Install Required Packages\n",
    "\n",
    "First, we need to install all the necessary packages for our notebook. Each package has a specific purpose:\n",
    "\n",
    "- **langchain**: Helps us build applications with language models\n",
    "- **langfuse**: Provides observability for our agent\n",
    "- **ragas**: Helps us evaluate our agent's performance\n",
    "- **chromadb**: A database for storing and searching vector embeddings\n",
    "- **docx2txt**: Converts Word documents to text\n",
    "- **boto3**: AWS SDK for Python, used to access AWS services and Use Amazon Bedrock Models\n",
    "- **strands**: Framework for building AI agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Create Vector Database from Restaurant Data\n",
    "\n",
    "A vector database stores text as numbers (vectors) that represent the meaning of the text. This allows us to search for similar meanings, not just exact word matches. For example, if we search for \"vegetarian food\", we might also find results about \"plant-based dishes\" even if those exact words weren't used.\n",
    "\n",
    "We'll create a vector database using restaurant data files in the `restaurant-data` folder. These files contain information about different restaurants, their menus, and specialties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for document loading\n",
    "import os  # For working with files and directories\n",
    "import docx2txt  # For converting Word documents to text\n",
    "from langchain.document_loaders import TextLoader  # For loading text documents\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # For splitting text into chunks\n",
    "from langchain.schema.document import Document  # For creating document objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load DOCX files\n",
    "def load_docx(file_path):\n",
    "    \"\"\"\n",
    "    This function takes a Word document (.docx) file path and converts it to text.\n",
    "    It then creates a Document object that can be used by our vector database.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the DOCX file\n",
    "        \n",
    "    Returns:\n",
    "        A Document object containing the text and metadata\n",
    "    \"\"\"\n",
    "    # Extract text from the DOCX file\n",
    "    text = docx2txt.process(file_path)\n",
    "    \n",
    "    # Create a Document object with the text and source information\n",
    "    return Document(page_content=text, metadata={\"source\": file_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all restaurant data files\n",
    "restaurant_data_dir = './restaurant-data/'  # Directory containing restaurant data files\n",
    "documents = []  # Empty list to store our documents\n",
    "\n",
    "# Loop through all files in the restaurant data directory\n",
    "for filename in os.listdir(restaurant_data_dir):\n",
    "    # Only process .docx files and skip temporary files (those starting with ~)\n",
    "    if filename.endswith('.docx') and not filename.startswith('~'):\n",
    "        file_path = os.path.join(restaurant_data_dir, filename)\n",
    "        try:\n",
    "            # Load the document and add it to our list\n",
    "            doc = load_docx(file_path)\n",
    "            documents.append(doc)\n",
    "            print(f\"Loaded: {filename}\")\n",
    "        except Exception as e:\n",
    "            # If there's an error, print it but continue with other files\n",
    "            print(f\"Error loading {filename}: {e}\")\n",
    "\n",
    "print(f\"Total documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into chunks\n",
    "# We split large documents into smaller chunks to make search more effective\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Each chunk will be about 500 characters\n",
    "    chunk_overlap=100  # Chunks will overlap by 100 characters to maintain context\n",
    ")\n",
    "\n",
    "# Apply the splitter to our documents\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(splits)} document chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Set up embeddings\n",
    "\n",
    "Embeddings are the mathematical representations (vectors) of text. We need to convert our text chunks into these vectors so they can be searched efficiently. For simplicity, we'll use a model from Amazon Bedrock, but in a production environment, you might want to use other embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for embeddings\n",
    "import boto3  # AWS SDK for Python\n",
    "from langchain_aws import BedrockEmbeddings  # For using Amazon Bedrock embeddings\n",
    "\n",
    "# Create a client for Amazon Bedrock\n",
    "# This allows us to communicate with the Bedrock service\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "# Specify which embedding model to use\n",
    "# Titan is Amazon's embedding model that converts text to vectors\n",
    "bedrock_embedding_model_id = 'amazon.titan-embed-text-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embedding model object\n",
    "# This will be used to convert our text to vectors\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=bedrock_client,\n",
    "    model_id=bedrock_embedding_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and persist the vector database\n",
    "from langchain_chroma import Chroma  # Chroma is a vector database\n",
    "\n",
    "# Define the directory where we'll save our vector database\n",
    "# This allows us to reuse it later without recreating it\n",
    "persist_directory = './restaurant-vectordb/'\n",
    "\n",
    "# Create the vector database from our document chunks\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,  # Our document chunks\n",
    "    embedding=embedding_model,  # The embedding model to use\n",
    "    persist_directory=persist_directory  # Where to save the database\n",
    ")\n",
    "\n",
    "print(\"Vector database created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Test the Vector Database\n",
    "\n",
    "Now that we've created our vector database, let's test it with a simple query to make sure it works correctly. We'll search for vegetarian options and see what results we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a simple query\n",
    "query = \"What vegetarian options are available?\"  # Our test question\n",
    "results = vectordb.similarity_search(query, k=3)  # Get the top 3 most relevant results\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"\\nTop 3 results:\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(f\"Source: {doc.metadata['source']}\")  # Which restaurant this is from\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")  # Show the first 200 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Set Up LangFuse for Observability\n",
    "\n",
    "**What is LangFuse?**\n",
    "\n",
    "LangFuse is like a dashboard for your AI agent. It helps you see what's happening inside your agent - what questions it's getting, how it's thinking about them, and what answers it's giving. This is incredibly useful for debugging and improving your agent.\n",
    "\n",
    "Now, let's configure LangFuse for observability. You'll need to create a LangFuse account and get your API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Create a new project in Langfuse](https://catalog.us-east-1.prod.workshops.aws/workshops/33f099a6-45a2-47d7-9e3c-a23a6568821e/en-US/01-fundamentals/18-agent-observability-and-evaluation#create-a-new-project-in-langfuse)\n",
    "\n",
    "1- Click on Sign-up to create a [langfuse account](https://us.cloud.langfuse.com/) or [Sign-in to an existing account](https://us.cloud.langfuse.com/).\n",
    "\n",
    "![image](image/1-langfuse.png)\n",
    "\n",
    "2- Create a New Organization and enter an orgnization name. Skip the Invite Members. Then create the project. \n",
    "\n",
    "![image](image/project.png)\n",
    "\n",
    "3- Copy and paste the Secret Key, Public Key and Host. Note you can also find the credentials in the Settings -> API Keys page.\n",
    "\n",
    "![api](image/api.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these with your LangFuse credentials\n",
    "# These keys are like passwords that let your code connect to LangFuse\n",
    "public_key = \"your-public-key\"  # Replace with your public key\n",
    "secret_key = \"your-secret-key\"  # Replace with your secret key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangFuse configuration\n",
    "from langfuse import Langfuse\n",
    "\n",
    "# Create a LangFuse client\n",
    "# This is the object we'll use to communicate with LangFuse\n",
    "langfuse = Langfuse(\n",
    "  public_key=public_key,\n",
    "  secret_key=secret_key,\n",
    "  host=\"https://us.cloud.langfuse.com\"  # For US region\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we can access the client\n",
    "from langfuse import get_client\n",
    " \n",
    "# Access the client directly\n",
    "langfuse = get_client(public_key=public_key)\n",
    " \n",
    "# Flush all pending observations\n",
    "# This ensures all data is sent to LangFuse\n",
    "langfuse.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ…  Create a Restaurant Recommendation Agent\n",
    "\n",
    "Now, let's create a Strands Agent that uses our vector database to provide restaurant recommendations. This agent will:\n",
    "1. Receive questions from users about restaurants\n",
    "2. Search our vector database for relevant information\n",
    "3. Generate helpful responses based on the search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for the agent\n",
    "from strands import Agent  # The main Agent class\n",
    "from strands.models.anthropic import AnthropicModel  # For using Claude model\n",
    "from strands.tools import tool  # For creating tools the agent can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the language model for our agent\n",
    "# We're using Claude 3 Sonnet from Anthropic\n",
    "model = AnthropicModel(\n",
    "    client_args={\n",
    "        \"api_key\": api_key,  # Replace with your API key\n",
    "    },\n",
    "    max_tokens=1028,  # Maximum response length\n",
    "    model_id=\"claude-sonnet-4-20250514\",  # Which model to use\n",
    "    params={\n",
    "        \"temperature\": 0.3,  # Lower temperature means more consistent, focused responses\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retrieval tool using our vector database\n",
    "# This tool will allow our agent to search for restaurant information\n",
    "\n",
    "@tool\n",
    "def search_restaurants(query):\n",
    "    \"\"\"\n",
    "    Search for restaurant information based on cuisine, dietary preferences, location, or other criteria.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query about restaurants\n",
    "        \n",
    "    Returns:\n",
    "        str: Relevant information about restaurants matching the query\n",
    "    \"\"\"\n",
    "    # Load the persisted vector database\n",
    "    loaded_vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)\n",
    "    \n",
    "    # Perform a similarity search\n",
    "    results = loaded_vectordb.similarity_search(query, k=3)\n",
    "    \n",
    "    # Format the results\n",
    "    formatted_results = \"\"\n",
    "    for i, doc in enumerate(results):\n",
    "        restaurant_name = os.path.basename(doc.metadata['source']).replace('.docx', '')\n",
    "        formatted_results += f\"Restaurant: {restaurant_name}\\n\"\n",
    "        formatted_results += f\"Information: {doc.page_content}\\n\\n\"\n",
    "    \n",
    "    return formatted_results if formatted_results else \"No relevant information found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the restaurant recommendation agent\n",
    "import uuid  # For generating unique IDs\n",
    "\n",
    "restaurant_agent = Agent(\n",
    "    name = \"Restaurant Recommendation Agent\",\n",
    "    model=model,\n",
    "    tools=[search_restaurants],  # Give the agent access to our search tool\n",
    "    system_prompt=\"\"\"You are a helpful restaurant recommendation assistant. \n",
    "    Use the search_restaurants tool to find information about restaurants based on user queries.\n",
    "    Provide detailed recommendations based on the search results.\n",
    "    If asked about restaurants that aren't in the database, politely explain that you can only provide information about restaurants in your database.\n",
    "    Always be friendly, helpful, and concise in your responses.\n",
    "    \"\"\",\n",
    "    record_direct_tool_call = True,  # Record when tools are used\n",
    "    trace_attributes={\n",
    "        \"session.id\": str(uuid.uuid4()),  # Generate a unique session ID\n",
    "        \"user.id\": \"user-email-example@domain.com\",  # Example user ID\n",
    "        \"langfuse.tags\": [\n",
    "            \"Agent-SDK-Example\",\n",
    "            \"Strands-Project-Demo\",\n",
    "            \"Observability-Tutorial\"\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ…  Test the Agent with Tracing\n",
    "\n",
    "Now let's test our agent with a simple query and see how it performs. The agent will use the search tool to find relevant information and then generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent with a simple query\n",
    "response = restaurant_agent(\"I'm looking for a restaurant with good vegetarian options. Any recommendations?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Review the traces\n",
    "\n",
    "After running the agent, you can review the traces in LangFuse:\n",
    "\n",
    "1. Go to the tracing menu in your LangFuse project\n",
    "2. Select the trace you want to view\n",
    "3. Examine how the agent processed the request, what tools it used, and what response it generated\n",
    "\n",
    "This gives you visibility into how your agent is working and helps you identify any issues or areas for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Set Up RAGAS for Evaluation\n",
    "\n",
    "Now, let's use RAGAS to evaluate the quality of our agent's responses. RAGAS (Retrieval Augmented Generation Assessment) is a framework for evaluating RAG systems.\n",
    "\n",
    "### What is RAGAS?\n",
    "\n",
    "RAGAS helps us measure how well our agent is performing by looking at different aspects of its responses:\n",
    "- Is the information accurate?\n",
    "- Is it relevant to the user's question?\n",
    "- Is it using the right tools?\n",
    "- Is it communicating in a friendly way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RAGAS libraries\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "# Set up the evaluator LLM (we'll use the same model as our agent)\n",
    "evaluator_llm = LangchainLLMWrapper(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define RAGAS Metrics\n",
    "\n",
    "We'll define several metrics to evaluate different aspects of our agent's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import AspectCritic\n",
    "\n",
    "# Metric to check if the agent fulfills all user requests\n",
    "request_completeness = AspectCritic(\n",
    "    name=\"Request Completeness\",\n",
    "    llm=evaluator_llm,\n",
    "    definition=(\n",
    "        \"Return 1 if the agent completely fulfills all the user requests with no omissions. \"\n",
    "        \"otherwise, return 0.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Metric to assess if the AI's communication aligns with the desired brand voice\n",
    "brand_tone = AspectCritic(\n",
    "    name=\"Brand Voice Metric\",\n",
    "    llm=evaluator_llm,\n",
    "    definition=(\n",
    "        \"Return 1 if the AI's communication is friendly, approachable, helpful, clear, and concise; \"\n",
    "        \"otherwise, return 0.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Tool usage effectiveness metric\n",
    "tool_usage_effectiveness = AspectCritic(\n",
    "    name=\"Tool Usage Effectiveness\",\n",
    "    llm=evaluator_llm,\n",
    "    definition=(\n",
    "        \"Return 1 if the agent appropriately used available tools to fulfill the user's request \"\n",
    "        \"(such as using retrieve for menu questions and current_time for time questions). \"\n",
    "        \"Return 0 if the agent failed to use appropriate tools or used unnecessary tools.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Tool selection appropriateness metric\n",
    "tool_selection_appropriateness = AspectCritic(\n",
    "    name=\"Tool Selection Appropriateness\",\n",
    "    llm=evaluator_llm,\n",
    "    definition=(\n",
    "        \"Return 1 if the agent selected the most appropriate tools for the task. \"\n",
    "        \"Return 0 if better tool choices were available or if unnecessary tools were selected.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also define a rubric score metric to evaluate how the agent handles situations where requested items aren't available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import RubricsScore\n",
    "\n",
    "# Define a rubric for evaluating recommendations\n",
    "rubrics = {\n",
    "    \"score-1_description\": (\n",
    "        \"\"\"The item requested by the customer is not present in the menu and no \n",
    "        recommendations were made.\"\"\"\n",
    "    ),\n",
    "    \"score0_description\": (\n",
    "        \"Either the item requested by the customer is present in the menu, \"\n",
    "        \"or the conversation does not include any \"\n",
    "        \"food or menu inquiry (e.g., booking, cancellation). \"\n",
    "        \"This score applies regardless of whether any recommendation was \"\n",
    "        \"provided.\"\n",
    "    ),\n",
    "    \"score1_description\": (\n",
    "        \"The item requested by the customer is not present in the menu \"\n",
    "        \"and a recommendation was provided.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Create the recommendations metric\n",
    "recommendations = RubricsScore(rubrics=rubrics, llm=evaluator_llm, name=\"Recommendations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's define metrics to evaluate the RAG (Retrieval-Augmented Generation) aspects of our agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import ContextRelevance, ResponseGroundedness \n",
    "\n",
    "# Context relevance measures how well the retrieved contexts address the user's query\n",
    "context_relevance = ContextRelevance(llm=evaluator_llm)\n",
    "\n",
    "# Response groundedness determines if the response is supported by the provided contexts\n",
    "response_groundedness = ResponseGroundedness(llm=evaluator_llm)\n",
    "\n",
    "metrics=[context_relevance, response_groundedness]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate the Agent and Send Results to LangFuse\n",
    "\n",
    "Now we'll create functions to evaluate our agent and send the results back to LangFuse:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining helper functions\n",
    "Now that we have defined our evaluation metrics, let's create some helper functions to help us processign the trace components for evaluation.\n",
    "\n",
    "### Extracting Components from Traces\n",
    "Now we will create a couple of functions to extract the necessary components from a Langfuse trace for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from ragas.dataset_schema import (\n",
    "    SingleTurnSample,\n",
    "    MultiTurnSample,\n",
    "    EvaluationDataset\n",
    ")\n",
    "from ragas import evaluate\n",
    "\n",
    "def extract_span_components(trace):\n",
    "    \"\"\"Extract user queries, agent responses, retrieved contexts \n",
    "    and tool usage from a Langfuse trace\"\"\"\n",
    "    user_inputs = []\n",
    "    agent_responses = []\n",
    "    retrieved_contexts = []\n",
    "    tool_usages = []\n",
    "\n",
    "    # Get basic information from trace\n",
    "    if hasattr(trace, 'input') and trace.input is not None:\n",
    "        if isinstance(trace.input, dict) and 'args' in trace.input:\n",
    "            if trace.input['args'] and len(trace.input['args']) > 0:\n",
    "                user_inputs.append(str(trace.input['args'][0]))\n",
    "        elif isinstance(trace.input, str):\n",
    "            user_inputs.append(trace.input)\n",
    "        else:\n",
    "            user_inputs.append(str(trace.input))\n",
    "\n",
    "    if hasattr(trace, 'output') and trace.output is not None:\n",
    "        if isinstance(trace.output, str):\n",
    "            agent_responses.append(trace.output)\n",
    "        else:\n",
    "            agent_responses.append(str(trace.output))\n",
    "\n",
    "    # Try to get contexts from observations and tool usage details\n",
    "    try:\n",
    "        for obsID in trace.observations:\n",
    "            print (f\"Getting Observation {obsID}\")\n",
    "            observations = langfuse.api.observations.get(obsID)\n",
    "\n",
    "            for obs in observations:\n",
    "                # Extract tool usage information\n",
    "                if hasattr(obs, 'name') and obs.name:\n",
    "                    tool_name = str(obs.name)\n",
    "                    tool_input = obs.input if hasattr(obs, 'input') and obs.input else None\n",
    "                    tool_output = obs.output if hasattr(obs, 'output') and obs.output else None\n",
    "                    tool_usages.append({\n",
    "                        \"name\": tool_name,\n",
    "                        \"input\": tool_input,\n",
    "                        \"output\": tool_output\n",
    "                    })\n",
    "                    # Specifically capture retrieved contexts\n",
    "                    if 'retrieve' in tool_name.lower() and tool_output:\n",
    "                        retrieved_contexts.append(str(tool_output))\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching observations: {e}\")\n",
    "\n",
    "    # Extract tool names from metadata if available\n",
    "    if hasattr(trace, 'metadata') and trace.metadata:\n",
    "        if 'attributes' in trace.metadata:\n",
    "            attributes = trace.metadata['attributes']\n",
    "            if 'agent.tools' in attributes:\n",
    "                available_tools = attributes['agent.tools']\n",
    "    return {\n",
    "        \"user_inputs\": user_inputs,\n",
    "        \"agent_responses\": agent_responses,\n",
    "        \"retrieved_contexts\": retrieved_contexts,\n",
    "        \"tool_usages\": tool_usages,\n",
    "        \"available_tools\": available_tools if 'available_tools' in locals() else []\n",
    "    }\n",
    "\n",
    "\n",
    "def fetch_traces(batch_size=10, lookback_hours=24, tags=None):\n",
    "    \"\"\"Fetch traces from Langfuse based on specified criteria\"\"\"\n",
    "    # Calculate time range\n",
    "    end_time = datetime.now()\n",
    "    start_time = end_time - timedelta(hours=lookback_hours)\n",
    "    print(f\"Fetching traces from {start_time} to {end_time}\")\n",
    "    # Fetch traces\n",
    "    if tags:\n",
    "        traces = langfuse.api.trace.list(\n",
    "            limit=batch_size,\n",
    "            tags=tags,\n",
    "            from_timestamp=start_time,\n",
    "            to_timestamp=end_time\n",
    "        ).data\n",
    "    else:\n",
    "        traces = langfuse.api.trace.list(\n",
    "            limit=batch_size,\n",
    "            from_timestamp=start_time,\n",
    "            to_timestamp=end_time\n",
    "        ).data\n",
    "    \n",
    "    print(f\"Fetched {len(traces)} traces\")\n",
    "    return traces\n",
    "\n",
    "def process_traces(traces):\n",
    "    \"\"\"Process traces into samples for RAGAS evaluation\"\"\"\n",
    "    single_turn_samples = []\n",
    "    multi_turn_samples = []\n",
    "    trace_sample_mapping = []\n",
    "    \n",
    "    for trace in traces:\n",
    "        # Extract components\n",
    "        components = extract_span_components(trace)\n",
    "        \n",
    "        # Add tool usage information to the trace for evaluation\n",
    "        tool_info = \"\"\n",
    "        if components[\"tool_usages\"]:\n",
    "            tool_info = \"Tools used: \" + \", \".join([t[\"name\"] for t in components[\"tool_usages\"] if \"name\" in t])\n",
    "            \n",
    "        # Convert to RAGAS samples\n",
    "        if components[\"user_inputs\"]:\n",
    "            # For single turn with context, create a SingleTurnSample\n",
    "            if components[\"retrieved_contexts\"]:\n",
    "                single_turn_samples.append(\n",
    "                    SingleTurnSample(\n",
    "                        user_input=components[\"user_inputs\"][0],\n",
    "                        response=components[\"agent_responses\"][0] if components[\"agent_responses\"] else \"\",\n",
    "                        retrieved_contexts=components[\"retrieved_contexts\"],\n",
    "                        # Add metadata for tool evaluation\n",
    "                        metadata={\n",
    "                            \"tool_usages\": components[\"tool_usages\"],\n",
    "                            \"available_tools\": components[\"available_tools\"],\n",
    "                            \"tool_info\": tool_info\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "                trace_sample_mapping.append({\n",
    "                    \"trace_id\": trace.id, \n",
    "                    \"type\": \"single_turn\", \n",
    "                    \"index\": len(single_turn_samples)-1\n",
    "                })\n",
    "            \n",
    "            # For regular conversation (single or multi-turn)\n",
    "            else:\n",
    "                messages = []\n",
    "                for i in range(max(len(components[\"user_inputs\"]), len(components[\"agent_responses\"]))):\n",
    "                    if i < len(components[\"user_inputs\"]):\n",
    "                        messages.append({\"role\": \"user\", \"content\": components[\"user_inputs\"][i]})\n",
    "                    if i < len(components[\"agent_responses\"]):\n",
    "                        messages.append({\n",
    "                            \"role\": \"assistant\", \n",
    "                            \"content\": components[\"agent_responses\"][i] + \"\\n\\n\" + tool_info\n",
    "                        })\n",
    "                \n",
    "                multi_turn_samples.append(\n",
    "                    MultiTurnSample(\n",
    "                        user_input=messages,\n",
    "                        metadata={\n",
    "                            \"tool_usages\": components[\"tool_usages\"],\n",
    "                            \"available_tools\": components[\"available_tools\"]\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "                trace_sample_mapping.append({\n",
    "                    \"trace_id\": trace.id, \n",
    "                    \"type\": \"multi_turn\", \n",
    "                    \"index\": len(multi_turn_samples)-1\n",
    "                })\n",
    "    \n",
    "    return {\n",
    "        \"single_turn_samples\": single_turn_samples,\n",
    "        \"multi_turn_samples\": multi_turn_samples,\n",
    "        \"trace_sample_mapping\": trace_sample_mapping\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting evaluation functions\n",
    "Next we will set some support evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_samples(single_turn_samples, trace_sample_mapping):\n",
    "    \"\"\"Evaluate RAG-based samples and push scores to Langfuse\"\"\"\n",
    "    if not single_turn_samples:\n",
    "        print(\"No single-turn samples to evaluate\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Evaluating {len(single_turn_samples)} single-turn samples with RAG metrics\")\n",
    "    rag_dataset = EvaluationDataset(samples=single_turn_samples)\n",
    "    rag_results = evaluate(\n",
    "        dataset=rag_dataset,\n",
    "        metrics=[context_relevance, response_groundedness]\n",
    "    )\n",
    "    rag_df = rag_results.to_pandas()\n",
    "    \n",
    "    # Push RAG scores back to Langfuse\n",
    "    for mapping in trace_sample_mapping:\n",
    "        if mapping[\"type\"] == \"single_turn\":\n",
    "            sample_index = mapping[\"index\"]\n",
    "            trace_id = mapping[\"trace_id\"]\n",
    "            \n",
    "            if sample_index < len(rag_df):\n",
    "                # Use actual column names from DataFrame\n",
    "                for metric_name in rag_df.columns:\n",
    "                    if metric_name not in ['user_input', 'response', 'retrieved_contexts']:\n",
    "                        try:\n",
    "                            metric_value = float(rag_df.iloc[sample_index][metric_name])\n",
    "                            langfuse.create_score(\n",
    "                                trace_id=trace_id,\n",
    "                                name=f\"rag_{metric_name}\",\n",
    "                                value=metric_value\n",
    "                            )\n",
    "                            print(f\"Added score rag_{metric_name}={metric_value} to trace {trace_id}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error adding RAG score: {e}\")\n",
    "    \n",
    "    return rag_df\n",
    "\n",
    "def evaluate_conversation_samples(multi_turn_samples, trace_sample_mapping):\n",
    "    \"\"\"Evaluate conversation-based samples and push scores to Langfuse\"\"\"\n",
    "    if not multi_turn_samples:\n",
    "        print(\"No multi-turn samples to evaluate\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Evaluating {len(multi_turn_samples)} multi-turn samples with conversation metrics\")\n",
    "    conv_dataset = EvaluationDataset(samples=multi_turn_samples)\n",
    "    conv_results = evaluate(\n",
    "        dataset=conv_dataset,\n",
    "        metrics=[\n",
    "            request_completeness, \n",
    "            recommendations,\n",
    "            brand_tone,\n",
    "            tool_usage_effectiveness,\n",
    "            tool_selection_appropriateness\n",
    "        ]\n",
    "        \n",
    "    )\n",
    "    conv_df = conv_results.to_pandas()\n",
    "    \n",
    "    # Push conversation scores back to Langfuse\n",
    "    for mapping in trace_sample_mapping:\n",
    "        if mapping[\"type\"] == \"multi_turn\":\n",
    "            sample_index = mapping[\"index\"]\n",
    "            trace_id = mapping[\"trace_id\"]\n",
    "            \n",
    "            if sample_index < len(conv_df):\n",
    "                for metric_name in conv_df.columns:\n",
    "                    if metric_name not in ['user_input']:\n",
    "                        try:\n",
    "                            metric_value = float(conv_df.iloc[sample_index][metric_name])\n",
    "                            if pd.isna(metric_value):\n",
    "                                metric_value = 0.0\n",
    "                            langfuse.create_score(\n",
    "                                trace_id=trace_id,\n",
    "                                name=metric_name,\n",
    "                                value=metric_value\n",
    "                            )\n",
    "                            print(f\"Added score {metric_name}={metric_value} to trace {trace_id}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error adding conversation score: {e}\")\n",
    "    \n",
    "    return conv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data\n",
    "Finally, we will create a function to save the data in CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(rag_df=None, conv_df=None, output_dir=\"evaluation_results\"):\n",
    "    \"\"\"Save evaluation results to CSV files\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    if rag_df is not None and not rag_df.empty:\n",
    "        rag_file = os.path.join(output_dir, f\"rag_evaluation_{timestamp}.csv\")\n",
    "        rag_df.to_csv(rag_file, index=False)\n",
    "        print(f\"RAG evaluation results saved to {rag_file}\")\n",
    "        results[\"rag_file\"] = rag_file\n",
    "    \n",
    "    if conv_df is not None and not conv_df.empty:\n",
    "        conv_file = os.path.join(output_dir, f\"conversation_evaluation_{timestamp}.csv\")\n",
    "        conv_df.to_csv(conv_file, index=False)\n",
    "        print(f\"Conversation evaluation results saved to {conv_file}\")\n",
    "        results[\"conv_file\"] = conv_file\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the main Evaluation Function\n",
    "We will now create the main function that fetches traces from Langfuse, processes them, runs Ragas evaluations, and pushes scores back to Langfuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_traces(batch_size=10, lookback_hours=24, tags=None, save_csv=False):\n",
    "    \"\"\"Main function to fetch traces, evaluate them with RAGAS, and push scores back to Langfuse\"\"\"\n",
    "    # Fetch traces from Langfuse\n",
    "    traces = fetch_traces(batch_size, lookback_hours, tags)\n",
    "    \n",
    "    if not traces:\n",
    "        print(\"No traces found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Process traces into samples\n",
    "    processed_data = process_traces(traces)\n",
    "    \n",
    "    # Evaluate the samples\n",
    "    rag_df = evaluate_rag_samples(\n",
    "        processed_data[\"single_turn_samples\"], \n",
    "        processed_data[\"trace_sample_mapping\"]\n",
    "    )\n",
    "    \n",
    "    conv_df = evaluate_conversation_samples(\n",
    "        processed_data[\"multi_turn_samples\"], \n",
    "        processed_data[\"trace_sample_mapping\"]\n",
    "    )\n",
    "    \n",
    "    # Save results to CSV if requested\n",
    "    if save_csv:\n",
    "        save_results_to_csv(rag_df, conv_df)\n",
    "    \n",
    "    return {\n",
    "        \"rag_results\": rag_df,\n",
    "        \"conversation_results\": conv_df\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    results = evaluate_traces(\n",
    "        lookback_hours=2,\n",
    "        batch_size=20,\n",
    "        tags=[\"Agent-SDK-Example\"],\n",
    "        save_csv=True\n",
    "    )\n",
    "    \n",
    "    # Access results if needed for further analysis\n",
    "    if results:\n",
    "        if \"rag_results\" in results and results[\"rag_results\"] is not None:\n",
    "            print(\"\\nRAG Evaluation Summary:\")\n",
    "            print(results[\"rag_results\"].describe())\n",
    "            \n",
    "        if \"conversation_results\" in results and results[\"conversation_results\"] is not None:\n",
    "            print(\"\\nConversation Evaluation Summary:\")\n",
    "            print(results[\"conversation_results\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "After running this evaluation pipeline:\n",
    "\n",
    "- Check your Langfuse dashboard to see the evaluation scores\n",
    "- Analyze trends in agent performance over time\n",
    "- Identify areas for improvement in your agent's responses by customizing Strand agent\n",
    "- Consider setting up automatic notifications for low-scoring interactions, you can setup a cron job or other events to run a periodic evaluation job\n",
    "\n",
    "Code based in the [Observability-and-Evaluation-sample.ipynb](https://github.com/strands-agents/samples/blob/main/01-tutorials/01-fundamentals/08-observability-and-evaluation/Observability-and-Evaluation-sample.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
